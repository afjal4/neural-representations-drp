{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 训练可学习 RBF kernel 的完整脚本\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"device:\", device)\n",
    "\n",
    "# ---------- 模型 ----------\n",
    "class EmbedNet(nn.Module):\n",
    "    \"\"\"\n",
    "    把输入 embedding 映射到新的表示空间。\n",
    "    你可以把它换成 nn.Linear(in_features, out_features, bias=False) 来做线性变换。\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_in=300, dim_hidden=300, use_nonlinear=True):\n",
    "        super().__init__()\n",
    "        if use_nonlinear:\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(dim_in, dim_hidden, bias=False),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(dim_hidden, dim_hidden, bias=False)\n",
    "            )\n",
    "        else:\n",
    "            self.net = nn.Linear(dim_in, dim_hidden, bias=False)\n",
    "        # learnable log_gamma for positivity\n",
    "        self.log_gamma = nn.Parameter(torch.tensor(0.0))  # gamma = exp(log_gamma)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# ---------- 距离 / RBF 计算 ----------\n",
    "def pairwise_sq_dists_torch(x):\n",
    "    # x: (N, d) tensor\n",
    "    # returns (N, N) matrix of squared euclidean distances\n",
    "    x2 = (x * x).sum(dim=1, keepdim=True)  # (N,1)\n",
    "    dist2 = x2 + x2.t() - 2.0 * (x @ x.t())\n",
    "    return torch.clamp(dist2, min=0.0)\n",
    "\n",
    "def rbf_from_Z(Z, log_gamma):\n",
    "    # Z: (N,d) tensor (not necessary to be normalized)\n",
    "    gamma = torch.exp(log_gamma)\n",
    "    sqd = pairwise_sq_dists_torch(Z)\n",
    "    K = torch.exp(-gamma * sqd)\n",
    "    return K\n",
    "\n",
    "# ---------- 辅助函数（评估 / 可视化 / purity） ----------\n",
    "def make_D_from_labels_torch(y):\n",
    "    y = y.view(-1,1)\n",
    "    D = (y == y.t()).float()\n",
    "    return D\n",
    "\n",
    "def purity_score(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    labels_true = np.unique(y_true)\n",
    "    labels_pred = np.unique(y_pred)\n",
    "    cost = np.zeros((labels_pred.size, labels_true.size), dtype=int)\n",
    "    for i, lp in enumerate(labels_pred):\n",
    "        for j, lt in enumerate(labels_true):\n",
    "            cost[i, j] = np.sum((y_pred == lp) & (y_true == lt))\n",
    "    row_ind, col_ind = linear_sum_assignment(-cost)\n",
    "    total = cost[row_ind, col_ind].sum()\n",
    "    return total / y_true.size\n",
    "\n",
    "def plot_heatmap(K, words=None, title=None, cmap='magma', figsize=(6,6), vmin=None, vmax=None):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(K, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    if words is not None:\n",
    "        plt.yticks(np.arange(len(words)), words, fontsize=8)\n",
    "        plt.xticks(np.arange(len(words)), words, rotation=90, fontsize=8)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ---------- 训练配置 ----------\n",
    "embed_dim = 300\n",
    "use_nonlinear = True    # 如果想试线性请设为 False\n",
    "kernel = EmbedNet(dim_in=embed_dim, dim_hidden=embed_dim, use_nonlinear=use_nonlinear).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(kernel.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "epochs = 100\n",
    "\n",
    "# optional: helper to compute avg pos/neg from K and labels (numpy)\n",
    "def avg_pos_neg_from_K(K_np, y_np, mask_diag=True):\n",
    "    N = K_np.shape[0]\n",
    "    D = (y_np[:,None] == y_np[None,:])\n",
    "    mask = np.ones((N,N), dtype=bool)\n",
    "    if mask_diag:\n",
    "        np.fill_diagonal(mask, False)\n",
    "    pos_mask = D & mask\n",
    "    neg_mask = (~D) & mask\n",
    "    pos_mean = K_np[pos_mask].mean() if pos_mask.sum() else np.nan\n",
    "    neg_mean = K_np[neg_mask].mean() if neg_mask.sum() else np.nan\n",
    "    return pos_mean, neg_mean\n",
    "\n",
    "# ---------- 训练 loop ----------\n",
    "loss_history = []\n",
    "pos_history = []\n",
    "neg_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    kernel.train()\n",
    "    total_loss = 0.0\n",
    "    total_pos = 0.0\n",
    "    total_neg = 0.0\n",
    "    count_batches = 0\n",
    "\n",
    "    for X_train, y_train, words_train in data_train:\n",
    "        X_train = X_train.to(device)            # (16, 300)\n",
    "        y_train = y_train.to(device)            # (16,)\n",
    "\n",
    "        Z = kernel(X_train)                     # (16, d)\n",
    "        # optional: normalize embeddings or not. For RBF we don't have to normalize.\n",
    "        # Z = F.normalize(Z, p=2, dim=1)\n",
    "        K = rbf_from_Z(Z, kernel.log_gamma)     # (16,16)\n",
    "\n",
    "        D = make_D_from_labels_torch(y_train).to(Z.device)\n",
    "        # mask out diagonal\n",
    "        N = K.size(0)\n",
    "        diag_mask = torch.eye(N, device=K.device).bool()\n",
    "        mask = ~diag_mask\n",
    "\n",
    "        # BCE loss between K (in (0,1)) and D (0/1)\n",
    "        loss = F.binary_cross_entropy(K[mask], D[mask])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(kernel.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # monitoring: compute pos/neg means (on CPU numpy)\n",
    "        with torch.no_grad():\n",
    "            K_np = K.detach().cpu().numpy()\n",
    "            y_np = y_train.detach().cpu().numpy()\n",
    "            pos_mean, neg_mean = avg_pos_neg_from_K(K_np, y_np, mask_diag=True)\n",
    "            if not np.isnan(pos_mean):\n",
    "                total_pos += pos_mean\n",
    "            total_neg += neg_mean\n",
    "            count_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / max(1, len(data_train))\n",
    "    avg_pos = total_pos / max(1, count_batches)\n",
    "    avg_neg = total_neg / max(1, count_batches)\n",
    "    loss_history.append(avg_loss)\n",
    "    pos_history.append(avg_pos)\n",
    "    neg_history.append(avg_neg)\n",
    "\n",
    "    if epoch % 5 == 0 or epoch == epochs - 1:\n",
    "        print(f\"epoch {epoch:03d} loss={avg_loss:.4f} pos_sim={avg_pos:.4f} neg_sim={avg_neg:.4f} gamma={float(torch.exp(kernel.log_gamma).item()):.4e}\")\n",
    "\n",
    "# ---------- 绘制 loss / pos-neg 曲线 ----------\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(loss_history, label='loss')\n",
    "plt.title(\"training loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(pos_history, label='pos_sim')\n",
    "plt.plot(neg_history, label='neg_sim')\n",
    "plt.title(\"pos / neg mean similarity\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ---------- 在 test 上可视化（第一个样本）和做聚类评估 ----------\n",
    "kernel.eval()\n",
    "X0, y0, words0 = data_test[0]\n",
    "with torch.inference_mode():\n",
    "    Z0 = kernel(X0.to(device))          # (16,d)\n",
    "    K0 = rbf_from_Z(Z0, kernel.log_gamma).cpu().numpy()\n",
    "    y0_np = y0.numpy()\n",
    "# raw heatmap\n",
    "plot_heatmap(K0, words=words0, title=\"test K_rbf (raw)\")\n",
    "\n",
    "# reorder by kmeans on rows of K for nicer block visualization\n",
    "n_clusters = len(np.unique(y0_np))\n",
    "km = KMeans(n_clusters=n_clusters, random_state=0).fit(K0)\n",
    "order = np.argsort(km.labels_)\n",
    "K_sorted = K0[np.ix_(order, order)]\n",
    "words_sorted = [words0[i] for i in order]\n",
    "plot_heatmap(K_sorted, words=words_sorted, title=\"test K_rbf (sorted by kmeans on rows)\")\n",
    "\n",
    "# quantitative clustering on embeddings (kmeans) and on K (spectral-like: kmeans on rows)\n",
    "Z0_np = Z0.detach().cpu().numpy()\n",
    "# clustering on embeddings\n",
    "km_emb = KMeans(n_clusters=n_clusters, random_state=0).fit(Z0_np)\n",
    "pred_emb = km_emb.labels_\n",
    "ari_emb = adjusted_rand_score(y0_np, pred_emb)\n",
    "nmi_emb = normalized_mutual_info_score(y0_np, pred_emb)\n",
    "pur_emb = purity_score(y0_np, pred_emb)\n",
    "\n",
    "# clustering on K rows\n",
    "km_K = KMeans(n_clusters=n_clusters, random_state=0).fit(K0)\n",
    "pred_K = km_K.labels_\n",
    "ari_K = adjusted_rand_score(y0_np, pred_K)\n",
    "nmi_K = normalized_mutual_info_score(y0_np, pred_K)\n",
    "pur_K = purity_score(y0_np, pred_K)\n",
    "\n",
    "print(\"Embedding KMeans -> ARI, NMI, Purity:\", ari_emb, nmi_emb, pur_emb)\n",
    "print(\"K-rows KMeans -> ARI, NMI, Purity:\", ari_K, nmi_K, pur_K)\n",
    "print(\"learned gamma:\", float(torch.exp(kernel.log_gamma).item()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
